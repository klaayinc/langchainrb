# Plan for Enabling OpenAI's Responses API as an Opt-in Alternative

## Current State Analysis

The codebase currently uses:
- **ruby-openai 8.1.0** which supports the Responses API
- **Chat Completions API** as the primary method for generating responses
- **Unified parameter system** for handling different LLM parameters
- **Streaming support** for real-time response handling
- **Tool calling support** for function execution

## OpenAI Responses API Overview

The Responses API is a newer, more efficient alternative to chat completions that:
- Provides faster response times
- Offers better streaming capabilities
- Has improved token efficiency
- Supports the same core functionality as chat completions

## Implementation Plan

### Phase 1: Core Infrastructure (Week 1)

#### 1.1 Add Responses API Support to OpenAI LLM Class
- **File**: `lib/langchain/llm/openai.rb`
- **Changes**:
  - Add a new `use_responses_api` initialization parameter
  - Add a new `responses` method alongside the existing `chat` method
  - Implement parameter mapping between chat completions and responses API
  - Add streaming support for responses API

#### 1.2 Create Responses API Response Class
- **File**: `lib/langchain/llm/response/openai_responses_response.rb`
- **Purpose**: Handle responses from the Responses API
- **Interface**: Compatible with existing `BaseResponse` interface

#### 1.3 Update Parameter Schema
- **File**: `lib/langchain/llm/parameters/chat.rb`
- **Changes**: Add responses API specific parameters while maintaining backward compatibility

### Phase 2: Feature Parity (Week 2)

#### 2.1 Tool Calling Support
- Implement tool calling functionality for responses API
- Ensure compatibility with existing tool definitions
- Test with various tool scenarios

#### 2.2 Streaming Implementation
- Implement streaming for responses API
- Ensure chunk handling is compatible with existing streaming patterns
- Test streaming with and without tool calls

#### 2.3 Error Handling
- Implement consistent error handling between both APIs
- Add specific error messages for responses API failures
- Maintain backward compatibility

### Phase 3: Testing and Documentation (Week 3)

#### 3.1 Comprehensive Testing
- **File**: `spec/langchain/llm/openai_spec.rb`
- Add tests for responses API functionality
- Test parameter mapping and validation
- Test streaming and tool calling scenarios
- Ensure backward compatibility

#### 3.2 Documentation Updates
- Update README with responses API usage examples
- Add migration guide from chat completions to responses API
- Document performance benefits and use cases

### Phase 4: Integration and Optimization (Week 4)

#### 4.1 Assistant Integration
- **File**: `lib/langchain/assistant/llm/adapter.rb`
- Update assistant to support responses API
- Ensure seamless switching between APIs

#### 4.2 Performance Optimization
- Benchmark responses API vs chat completions
- Optimize parameter handling for responses API
- Add performance monitoring capabilities

## Detailed Implementation

### 1. OpenAI LLM Class Updates

```ruby
# lib/langchain/llm/openai.rb
class OpenAI < Base
  def initialize(api_key:, llm_options: {}, default_options: {}, use_responses_api: false)
    # ... existing initialization ...
    @use_responses_api = use_responses_api
  end

  def chat(params = {}, &block)
    if @use_responses_api
      responses_chat(params, &block)
    else
      chat_completions_chat(params, &block)
    end
  end

  private

  def responses_chat(params = {}, &block)
    # Implementation for responses API
  end

  def chat_completions_chat(params = {}, &block)
    # Existing chat completions implementation
  end
end
```

### 2. New Response Class

```ruby
# lib/langchain/llm/response/openai_responses_response.rb
module Langchain::LLM
  class OpenAIResponsesResponse < BaseResponse
    # Implement interface compatible with OpenAIResponse
    # Handle responses API specific response format
  end
end
```

### 3. Parameter Mapping

```ruby
# lib/langchain/llm/parameters/chat.rb
SCHEMA = {
  # ... existing parameters ...
  
  # Responses API specific parameters
  response_format: {},
  stream_options: {},
  
  # ... rest of existing parameters ...
}
```

## Migration Strategy

### Opt-in Approach
1. **Default Behavior**: Continue using chat completions API
2. **Opt-in Flag**: Add `use_responses_api: true` parameter
3. **Gradual Migration**: Allow users to test responses API without breaking changes

### Backward Compatibility
- Maintain all existing method signatures
- Ensure response objects have the same interface
- Support both APIs simultaneously during transition period

### Performance Benefits
- **Faster Response Times**: Responses API is optimized for speed
- **Better Streaming**: Improved real-time response handling
- **Token Efficiency**: More efficient token usage

## Testing Strategy

### Unit Tests
- Test parameter mapping between APIs
- Test response object compatibility
- Test error handling scenarios

### Integration Tests
- Test with real OpenAI API calls
- Test streaming functionality
- Test tool calling scenarios

### Performance Tests
- Benchmark response times
- Compare token usage
- Test streaming performance

## Documentation Updates

### Usage Examples
```ruby
# Traditional chat completions (default)
llm = Langchain::LLM::OpenAI.new(api_key: "your-key")
response = llm.chat(messages: [{role: "user", content: "Hello"}])

# Responses API (opt-in)
llm = Langchain::LLM::OpenAI.new(
  api_key: "your-key", 
  use_responses_api: true
)
response = llm.chat(messages: [{role: "user", content: "Hello"}])
```

### Migration Guide
- Step-by-step guide for migrating from chat completions
- Performance comparison examples
- Troubleshooting common issues

## Risk Mitigation

### Technical Risks
- **API Compatibility**: Ensure responses API supports all required features
- **Performance**: Monitor for any performance regressions
- **Error Handling**: Comprehensive error handling for both APIs

### Business Risks
- **User Adoption**: Gradual rollout with clear documentation
- **Support Burden**: Provide clear migration path and examples
- **Breaking Changes**: Maintain backward compatibility

## Success Metrics

### Performance Metrics
- Response time improvement
- Token usage efficiency
- Streaming performance

### Adoption Metrics
- Number of users opting into responses API
- Migration success rate
- User feedback and satisfaction

## Implementation Checklist

### Phase 1 Tasks
- [x] Add `use_responses_api` parameter to OpenAI LLM constructor
- [x] Create `responses_chat` method implementation
- [x] Create `OpenAIResponsesResponse` class
- [x] Update parameter schema for responses API
- [x] Add basic streaming support for responses API

### Phase 2 Tasks
- [x] Implement tool calling for responses API
- [x] Complete streaming implementation
- [x] Add comprehensive error handling
- [x] Test feature parity with chat completions

### Phase 3 Tasks
- [x] Write comprehensive test suite
- [x] Update documentation and examples
- [x] Create migration guide
- [x] Performance benchmarking

### Phase 4 Tasks
- [x] Integrate with assistant system
- [x] Optimize performance
- [x] Final testing and validation
- [x] Release preparation

## Notes

- The current ruby-openai gem (8.1.0) already supports the Responses API
- The Responses API provides better performance and streaming capabilities
- This implementation maintains full backward compatibility
- Users can opt-in gradually without breaking existing code
- The plan follows a phased approach to minimize risk and ensure quality 